<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Containers | emodemo’s notes</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Containers" />
<meta name="author" content="emodemo" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="emodemo’s notes on development, fiction, science and other stuff." />
<meta property="og:description" content="emodemo’s notes on development, fiction, science and other stuff." />
<link rel="canonical" href="/dev/Containers.html" />
<meta property="og:url" content="/dev/Containers.html" />
<meta property="og:site_name" content="emodemo’s notes" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Containers" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"emodemo"},"@type":"WebPage","description":"emodemo’s notes on development, fiction, science and other stuff.","url":"/dev/Containers.html","headline":"Containers","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="emodemo's notes" /><script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script></head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">emodemo&#39;s notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/pages/dev.html">Development</a><a class="page-link" href="/pages/fiction.html">Fiction</a><a class="page-link" href="/pages/science.html">Science</a><a class="page-link" href="/pages/else.html">Other Stuff</a><a class="page-link" href="/pages/library.html">Library</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Containers</h1>
  </header>

  <div class="post-content">
    <!-- default one if nothing else is mentioned -->
<h2 id="other-useful-information">Other useful information</h2>

<ul>
  <li><a href="https://github.com/GoogleContainerTools/jib">auto-containerization - google jib</a></li>
  <li><a href="https://github.com/GoogleContainerTools/kaniko">Kaniko - image builder</a></li>
  <li><a href="https://github.com/ahmetb/kubectl-tree">kubectl - tree plugin</a></li>
  <li><a href="https://github.com/k8spatterns/examples">k8s patterns book sources</a></li>
  <li>https://learnk8s.io/production-best-practices</li>
  <li>https://learnk8s.io/research</li>
  <li>https://learnk8s.io/terraform-eks</li>
  <li><a href="https://argoproj.github.io/projects/argo">argo</a></li>
  <li><a href="https://www.jaegertracing.io/">jaeger</a></li>
  <li><a href="https://github.com/grafana/loki">loki</a></li>
  <li><a href="https://github.com/kubernetes/kops">kops</a></li>
  <li><a href="https://editor.cilium.io/">network policies editor cilium</a></li>
</ul>

<h2 id="docker">DOCKER</h2>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># run myTomcat image in bridge network</span>
<span class="o">&gt;</span> docker run <span class="nt">-it</span> <span class="nt">--net</span><span class="o">=</span>bridge myTomcat
<span class="c"># run myPostgreSQL in the same network as myTomcat</span>
<span class="o">&gt;</span> docker run <span class="nt">-it</span> <span class="nt">--net</span><span class="o">=</span>container:myTomcat myPostgreSQL

<span class="o">&gt;</span> <span class="nb">sudo </span>systemctl start docker.service <span class="c">## &lt;-- Start docker ##</span>
<span class="o">&gt;</span> <span class="nb">sudo </span>systemctl stop docker.service <span class="c">## &lt;-- Stop docker ##</span>
<span class="o">&gt;</span> <span class="nb">sudo </span>systemctl restart docker.service <span class="c">## &lt;-- Restart docker ##</span>
<span class="o">&gt;</span> <span class="nb">sudo </span>systemctl status docker.service <span class="c">## &lt;-- Get status of docker ##</span>

<span class="c"># insipred by https://btburnett.com/2017/03/remove-untagged-images-from-docker-for-windows.html</span>
<span class="o">&gt;</span> docker ps <span class="nt">-a</span> <span class="nt">-q</span> | % <span class="o">{</span> docker <span class="nb">rm</span> <span class="nv">$_</span> <span class="o">}</span> <span class="c">## remove all stopped containers on windows</span>
<span class="o">&gt;</span> docker images | ConvertFrom-String | where <span class="o">{</span><span class="nv">$_</span>.P2 <span class="nt">-eq</span> <span class="s2">"&lt;none&gt;"</span><span class="o">}</span> | % <span class="o">{</span> docker rmi <span class="nv">$_</span>.P3 <span class="o">}</span> <span class="c"># remove untagged images on windows</span>
</code></pre></div></div>

<ul>
  <li><a class="citation" href="#burns2019kubernetes">(Burns et al., 2019)</a></li>
  <li>Unlike <code class="language-plaintext highlighter-rouge">CMD</code>, the <code class="language-plaintext highlighter-rouge">RUN</code> instruction is actually used to build the image, by creating a new layer on top of the previous one which is committed.</li>
  <li>The <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code> specifies a command that will always be executed when the container starts. The CMD, on the other hand, specifies the arguments that will be fed to the <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code>. Docker has a default <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code> which is <code class="language-plaintext highlighter-rouge">/bin/sh -c</code> but does not have a default <code class="language-plaintext highlighter-rouge">CMD</code>.</li>
  <li>Unlike the <code class="language-plaintext highlighter-rouge">CMD</code> parameters, the <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code> command and parameters are not ignored when a Docker container runs with command-line parameters.</li>
  <li>If there are more than one <code class="language-plaintext highlighter-rouge">CMD</code> instruction in a Dockerfile, only the last one will take effect.</li>
  <li><code class="language-plaintext highlighter-rouge">EXPOSE</code> command followed by a port number allow incoming traffic to the container</li>
  <li>The fundamental difference between <code class="language-plaintext highlighter-rouge">VOLUME</code> and <code class="language-plaintext highlighter-rouge">-v</code> is this: <code class="language-plaintext highlighter-rouge">-v</code> will mount existing files from your operating system inside your Docker container and <code class="language-plaintext highlighter-rouge">VOLUME</code> will create a new, empty volume on your host and mount it inside your container.</li>
  <li>!! Each <code class="language-plaintext highlighter-rouge">LABEL</code> instruction creates a new layer. If your image has many labels, use the multiple form of the single <code class="language-plaintext highlighter-rouge">LABEL</code> instruction.</li>
</ul>

<h2 id="helm">HELM</h2>

<ul>
  <li>templates and package management tool</li>
</ul>

<h2 id="k8s">K8S</h2>

<ul>
  <li><a class="citation" href="#dinesh2018k8sbestpractices">(Dinesh, 2018)</a></li>
  <li><a class="citation" href="#gupta2017kubernetes">(Gupta, 2017)</a></li>
  <li><a class="citation" href="#noauthor0000oracle">(“Oracle Linux Container Services for Use with Kubernetes User’s Guide,” n.d.)</a></li>
  <li>K8S is dropping Docker for containerD, which is the Container Runtime used in Dokcer that is now extracted as a separate component. AWS, GoogleCloud are already using it in their managed K8S clusters.
    <ul>
      <li>Alternative container runtime is Cri-o, used by OpenShift.</li>
    </ul>
  </li>
</ul>

<h3 id="on-ubuntu">on Ubuntu</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">sudo </span>apt-get update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> apt-transport-https
<span class="o">&gt;</span> curl <span class="nt">-s</span> https://packages.cloud.google.com/apt/doc/apt-key.gpg | <span class="nb">sudo </span>apt-key add -
<span class="o">&gt;</span> <span class="nb">echo</span> <span class="s2">"deb https://apt.kubernetes.io/ kubernetes-xenial main"</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/apt/sources.list.d/kubernetes.list
<span class="o">&gt;</span> <span class="nb">sudo </span>apt-get update
<span class="o">&gt;</span> <span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> kubectl
<span class="o">&gt;</span> <span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> kubeadm kubelet
</code></pre></div></div>

<p><img src="pics\architecture_control_and_data_overview.jpg" alt="architecture" /></p>

<ul>
  <li>One or More API Servers: Entry point for REST / kubectl</li>
  <li>etcd: Distributed key/value store</li>
  <li>Controller-manager: Always evaluating current vs desired state</li>
  <li>Scheduler: Schedules pods to worker nodes</li>
  <li>kubelet: Acts as a conduit between the API server and the node</li>
  <li>kube-proxy: Manages IP translation and routing</li>
  <li><a class="citation" href="#krochmalski2017docker">(Krochmalski, 2017)</a> kube-proxy knows only UDP and TCP, does not understand HTTP, provides load balancing, and is just used to reach services.</li>
  <li>A Worker Node contains
    <ul>
      <li>Kubelet service to communicate with the Master Node</li>
      <li>Proxy acting as a network proxy and load balancer for a service on the node</li>
      <li>containerD - container runtime, preciously Docker Engine.</li>
    </ul>
  </li>
  <li>Worker Nodes are accessed thru Load Balancer</li>
  <li>multiple definitions within a single yaml are separated by —</li>
</ul>

<h3 id="kubectl">kubectl</h3>

<ul>
  <li><a class="citation" href="#burns2019kubernetes">(Burns et al., 2019)</a> kubectl + JSONPath to extract specific fields from objects (pod, daemonSet, deployment). As an example, this command will extract and print the IP address of the pod: <code class="language-plaintext highlighter-rouge">&gt; kubectl get pods my-pod -o jsonpath --template={.status.podIP}</code></li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> kubectl apply <span class="nt">-f</span> loanamort-pod.yaml
<span class="o">&gt;</span> kubectl run <span class="o">{</span>name<span class="o">}</span> <span class="nt">--image</span><span class="o">={</span>image<span class="o">}</span> <span class="nt">--port</span><span class="o">={</span>number<span class="o">}</span> <span class="c"># where the port is exposed within the cluster, not outside.</span>
<span class="o">&gt;</span> kubectl expose deployment <span class="o">{</span>deployment name<span class="o">}</span> <span class="nt">--name</span><span class="o">={</span>service name<span class="o">}</span> <span class="nt">--port</span><span class="o">{</span>number<span class="o">}</span> <span class="nt">--target-port</span><span class="o">={</span>number<span class="o">}</span> <span class="c"># where --port is where the service should listen, and --target-port is where the traffic should be directed to.</span>
<span class="o">&gt;</span> kubectl delete deployment/<span class="o">{</span>deployment name<span class="o">}</span> <span class="c"># deletes the resource-type/resource-name</span>
<span class="o">&gt;</span> kubectl create <span class="nt">-f</span> myConfigFile.yml <span class="c"># and</span>
<span class="o">&gt;</span> kubectl delete <span class="nt">-f</span> myConfigFile.yml <span class="c"># where the configuration file (and what is defined inside).</span>
<span class="o">&gt;</span> kubectl proxy <span class="c"># default access URL is: http://localhost:8001/api/v1/proxy/namespaces/default/{resource-type}/{resource-name}</span>
<span class="c"># example: http://localhost:8001/api/v1/proxy/namespaces/default/services/hello-wildfly-service/index.html</span>
<span class="o">&gt;</span> kubectl proxy <span class="nt">--address</span> 0.0.0.0 <span class="nt">--accept-hosts</span> <span class="s1">'.*'</span>
<span class="c"># http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</span>
<span class="o">&gt;</span> kubectl port-forward <span class="o">{</span>pod name<span class="o">}</span> <span class="o">{</span>port<span class="o">}</span>:<span class="o">{</span>port<span class="o">}</span>
<span class="c"># required for some debugging cases either with &gt;docker run ... --cap-add=SYS_PTRACE ... </span>
<span class="c"># or with k8s:</span>
  capabilities:
     add:
     - SYS_PTRACE
</code></pre></div></div>

<ul>
  <li><a class="citation" href="#burns2019kubernetes">(Burns et al., 2019)</a> Using Port Forwarding Later in the book, we’ll show how to expose a service to the world or other containers using load balancers, but oftentimes you simply want to access a specific Pod, even if it’s not serving traffic on the internet. To achieve this, you can use the port-forwarding support built into the Kubernetes API and command-line tools. When you run: <code class="language-plaintext highlighter-rouge">&gt; kubectl port-forward loanamort 8080:8080</code> a secure tunnel is created from your local machine, through the Kubernetes master, to the instance of the Pod running on one of the worker nodes. As long as the port-forward command is still running, you can access the Pod (in this case the kuard web interface) on http://localhost:8080.</li>
</ul>

<h3 id="pod--replicaset--deployment">Pod &amp; ReplicaSet &amp; Deployment</h3>

<ul>
  <li>Pod - smallest deployable unit that can be created, scheduled, managed</li>
  <li>created in namespace =&gt; containers inside share network namespace (ip and port), volume, networks stack, UTS namespace, and find each other on <code class="language-plaintext highlighter-rouge">localhost</code>.
    <ul>
      <li>have the same hostname (UTS namespace),</li>
      <li>and can communicate using native interprocess communication channels over System V IPC or POSIX message queues (IPC namespace).</li>
    </ul>
  </li>
  <li>ReplicaSet - Maintains the lifecycle and number of POD’s replicas running at one time.</li>
  <li>Deployment - responsible for creating and updating instances of your application. It is a higher level of abstraction; it manages ReplicaSets when doing Pod orchestration, creation, deletion, and updates <a class="citation" href="#krochmalski2017docker">(Krochmalski, 2017)</a>.</li>
  <li><strong>Canary Deployment</strong> - this technique can be implemented by creating a new ReplicaSet for the new container version (preferably using a Deployment) with a small replica count that can be used as the Canary instance. At this stage, the Service should direct some of the consumers to the updated Pod instances. Once we are confident that everything with new ReplicaSet works as expected, we scale a new ReplicaSet up, and the old ReplicaSet down to zero. In a way, we are performing a controlled and user-tested incremental rollout <a class="citation" href="#bilgin2019kubernetes">(Ibryam &amp; Hub, 2019)</a>.</li>
</ul>

<h3 id="ingress--service---a-logical-collection-of-pods-and-access-policy">Ingress &amp; Service - a logical collection of PODs, and access policy</h3>

<ul>
  <li><a class="citation" href="#krochmalski2017docker">(Krochmalski, 2017)</a> Each service is given its own IP address and port which remains constant for the lifetime of the service. Services have an integrated load-balancer that will distribute network traffic to all Pods. While a Pod’s life can be fragile as they are being spun up or down depending on your application needs, the service is a more constant concept.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">ClusterIP</code>, <code class="language-plaintext highlighter-rouge">NodePort</code>, <code class="language-plaintext highlighter-rouge">LoadBalancer</code>, <code class="language-plaintext highlighter-rouge">Headless</code> + <code class="language-plaintext highlighter-rouge">Endpoint</code>, <code class="language-plaintext highlighter-rouge">ClusterIP</code> with <code class="language-plaintext highlighter-rouge">ExternalName</code></li>
    </ul>
  </li>
  <li>Another potions is <code class="language-plaintext highlighter-rouge">Ingress</code>(see <a class="citation" href="#bilgin2019kubernetes">(Ibryam &amp; Hub, 2019)</a> for more details).</li>
  <li>Assuming a client knows the name of the Service it wants to access, it can reach the Service by a fully qualified domain name (FQDN) such as <code class="language-plaintext highlighter-rouge">random-generator.default.svc.cluster.local</code>. Here, <code class="language-plaintext highlighter-rouge">random-generator</code> is the name of the Service, <code class="language-plaintext highlighter-rouge">default</code> is the name of the namespace, <code class="language-plaintext highlighter-rouge">svc</code> indicates it is a Service resource, and <code class="language-plaintext highlighter-rouge">cluster.local</code> is the clusterspecific suffix. We can omit the cluster suffix if desired, and the namespace as well when accessing the Service from the same namespace.</li>
</ul>

<h3 id="job---creates-at-most-1-pods-to-ensure-n-of-them-complete-successfully">Job - creates at most 1+ PODs to ensure N of them complete successfully</h3>

<ul>
  <li>for short running processes</li>
  <li>Nonparallel (a single POD to finish), Parallel (N PODs finishes, or coordination work queue, or external service for terminate condition) and CronJob</li>
</ul>

<h3 id="volume---directory-accessible-to-the-containers">Volume - Directory accessible to the containers</h3>

<ul>
  <li>The lifecycle of a volume is tied to the pod that created it. Pods can store data on this volume and preserve data across container restarts. But the volume ceases to exist along with the pod. Moreover, pods are ephemeral and so may be rescheduled on a different host. This means the data cannot be stored on a host as well.</li>
  <li>Types:
    <ul>
      <li><strong>hostPath</strong>: A file or directory from the host node’s filesystem</li>
      <li><strong>nfs</strong>: Existing Network File System share</li>
      <li><strong>awsElasticBlockStore</strong>: An Amazon Web Service EBS volume</li>
      <li><strong>gcePersistentDisk</strong>: A Google Compute Engine persistent disk</li>
      <li><strong>secrets</strong></li>
      <li><strong>ConfigMap</strong></li>
      <li>…</li>
    </ul>
  </li>
</ul>

<h3 id="persistentvolume---cluster-scoped-storage-for-applications-that-require-long-lived-data">PersistentVolume - cluster-scoped storage for applications that require long-lived data.</h3>

<ul>
  <li>Provision</li>
</ul>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span> <span class="c1"># the volume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">couchbase-pv</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">amazonEBS</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">5Gi</span> <span class="c1"># the capacity</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">awsElasticBlockStore</span><span class="pi">:</span>
    <span class="na">volumeID</span><span class="pi">:</span> <span class="s">vol-0e04a9f45ad0cc01d</span> <span class="c1"># unique ID</span>
    <span class="na">fsType</span><span class="pi">:</span> <span class="s">ext4</span>
</code></pre></div></div>

<ul>
  <li>Request</li>
</ul>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span> <span class="c1"># the claim</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">couchbase-pvc</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">amazonEBS</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">3Gi</span>
</code></pre></div></div>

<ul>
  <li>Claim thtough a Pod</li>
</ul>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># pod.yml</span>
<span class="na">volumes</span><span class="pi">:</span>
 <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">couchbase-volume</span>
 <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
 <span class="na">claimName</span><span class="pi">:</span> <span class="s">couchbase-pvc</span> <span class="c1"># the claim match</span>
</code></pre></div></div>

<h3 id="auto-scaling">Auto-Scaling</h3>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">HorizontalPodAutoscaler</code> is prefered over the <code class="language-plaintext highlighter-rouge">VerticalPodAutoscaler</code>.</li>
  <li>Scales the numbers of pods in a replication controller, deployment, or replica based on observed CPU utilization.</li>
  <li>By default, the auto-scaler checks every 30 seconds and adjusts the number of replicas to match the observed average CPU utilization to the target specified by the user.</li>
  <li>The auto-scaler uses https://github.com/kubernetes/heapster to collect CPU utilization information, so it must be installed in the cluster for auto-scaling to work.</li>
  <li>Alternatively Service Mesh could be used - e.g. Knative (on top of Istio).</li>
</ul>

<h3 id="daemonset">DaemonSet</h3>

<h3 id="statefulset">StatefulSet</h3>

<ul>
  <li><a class="citation" href="#bilgin2019kubernetes">(Ibryam &amp; Hub, 2019)</a> Rather than referring to a predefined PVC, StatefulSets create PVCs by using volume <code class="language-plaintext highlighter-rouge">ClaimTemplates</code> on the fly during Pod creation. This mechanism allows every Pod to get its own dedicated PVC during initial creation as well as during scaling up by changing the replicas count of the <code class="language-plaintext highlighter-rouge">StatefulSets</code>.
    <ul>
      <li>PVCs are created and associated with the Pods, but … StatefulSets do not manage PVs in any way. The storage for the Pods must be provisioned in advance by an admin, or provisioned on-demand by a PV provisioner based on the requested storage class and ready for consumption by the stateful Pods.</li>
      <li>Note the asymmetric behavior here: scaling up a StatefulSet (increasing the replicas count) creates new Pods and associated PVCs. Moreover, scaling down deletes the Pods, but it does not delete any PVCs (nor PVs), which means the PVs cannot be recycled or deleted, and Kubernetes cannot free the storage. This behavior is by design and driven by the presumption that the storage of stateful applications is critical and that an accidental scale-down should not cause data loss. If you are sure the stateful application has been scaled down on purpose and has replicated/drained the data to other instances, you can delete the PVC manually, which allows subsequent PV recycling.</li>
    </ul>
  </li>
</ul>

<h3 id="helath-probes">Helath Probes</h3>

<ul>
  <li>Startup, Readiness, Liveness</li>
</ul>

<h3 id="namespace">Namespace</h3>

<ul>
  <li><a class="citation" href="#krochmalski2017docker">(Krochmalski, 2017)</a> A namespace functions as a grouping mechanism inside of Kubernetes. Pods, Volumes, ReplicaSets, and services can easily cooperate within a namespace, but the namespace provides an isolation from the other parts of the cluster</li>
  <li><strong>cross namespace communitcation</strong> - DNS is in the form of <code class="language-plaintext highlighter-rouge">&lt;Service Name&gt;.&lt;Namespace Name&gt;.svc.clister.local</code> and it is called with the <code class="language-plaintext highlighter-rouge">&lt;Service Name&gt;</code> only. So just add the namespace name to it.
    <ul>
      <li>with networkpolicies we can restrict calls from one namespace to another one</li>
    </ul>
  </li>
</ul>

<h3 id="scheduling-policies---match-a-pod-to-a-node">Scheduling policies - match a Pod to a Node</h3>

<ul>
  <li><a class="citation" href="#bilgin2019kubernetes">(Ibryam &amp; Hub, 2019)</a> Pods get assigned to nodes with certain capacities based on placement policies… As soon as a Pod is created that is not assigned to a node yet, it gets picked by the scheduler together with all the available nodes and the set of filtering and priority policies. In the first stage, the scheduler applies the filtering policies and removes all nodes that do not qualify based on the Pod’s criteria. In the second stage, the remaining nodes get ordered by weight. In the last stage the Pod gets a node assigned, which is the primary outcome of the scheduling process.<br />
<strong>In most cases, it is better to let the scheduler do the Pod-to-node assignment and not micromanage the placement logic.</strong> However, on some occasions, you may want to force the assignment of a Pod to a specific node or a group of nodes. This assignment can be done using a node selector. <code class="language-plaintext highlighter-rouge">.spec.nodeSelector</code> is Pod field and specifies a map of key-value pairs that must be present as labels on the node for the node to be eligible to run the Pod.</li>
  <li>Scheduler Policies - predicates and priorities.</li>
  <li>Node <code class="language-plaintext highlighter-rouge">affinity</code> and Pod <code class="language-plaintext highlighter-rouge">anti/affinity</code></li>
</ul>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># in the pod.xml</span>
<span class="na">spec</span><span class="pi">:</span>
 <span class="na">affinity</span><span class="pi">:</span>
  <span class="na">nodeAffinity</span><span class="pi">:</span> <span class="s">...</span>
  <span class="na">podAffinity</span><span class="pi">:</span> <span class="s">...</span>
  <span class="na">podAntiAffinity</span><span class="pi">:</span> <span class="s">...</span>
</code></pre></div></div>

<ul>
  <li><a class="citation" href="#bilgin2019kubernetes">(Ibryam &amp; Hub, 2019)</a> A more advanced feature that controls where Pods can be scheduled and are allowed to run is based on <code class="language-plaintext highlighter-rouge">taints</code> and <code class="language-plaintext highlighter-rouge">tolerations</code>. While node affinity is a property of Pods that allows them to choose nodes, <code class="language-plaintext highlighter-rouge">taints</code> and <code class="language-plaintext highlighter-rouge">tolerations</code> are the opposite. They allow the nodes to control which Pods should or should not be scheduled on them. A <code class="language-plaintext highlighter-rouge">taint</code> is a characteristic of the node, and when it is present, it prevents Pods from scheduling onto the node unless the Pod has toleration for the taint. In that sense, <code class="language-plaintext highlighter-rouge">taints</code> and <code class="language-plaintext highlighter-rouge">tolerations</code> can be considered as an <em>opt-in</em> to allow scheduling on nodes, which by default are not available for scheduling, whereas <code class="language-plaintext highlighter-rouge">affinity</code> rules are an <em>opt-out</em> by explicitly selecting on which nodes to run and thus exclude all the nonselected nodes.
    <ul>
      <li>There are hard taints that prevent scheduling on a node <code class="language-plaintext highlighter-rouge">(effect=NoSchedule)</code>, soft taints that try to avoid scheduling on a node <code class="language-plaintext highlighter-rouge">(effect=PreferNoSchedule)</code>, and taints that can evict already running Pods from a node <code class="language-plaintext highlighter-rouge">(effect=NoExecute)</code>.</li>
    </ul>
  </li>
</ul>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Taineted Node</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Node</span>
<span class="na">metadata</span><span class="pi">:</span>
 <span class="na">name</span><span class="pi">:</span> <span class="s">master</span>
<span class="na">spec</span><span class="pi">:</span>
 <span class="na">taints</span><span class="pi">:</span>
 <span class="pi">-</span> <span class="na">effect</span><span class="pi">:</span> <span class="s">NoSchedule</span> <span class="c1"># Taint on a node’s spec to mark this node as not available for scheduling except when a Pod tolerates this taint</span>
 <span class="na">key</span><span class="pi">:</span> <span class="s">node-role.kubernetes.io/master</span>

<span class="c1"># Pod tolerationg Node taints</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
 <span class="na">name</span><span class="pi">:</span> <span class="s">random-generator</span>
<span class="na">spec</span><span class="pi">:</span>
 <span class="na">containers</span><span class="pi">:</span>
 <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">k8spatterns/random-generator:1.0</span>
 <span class="na">name</span><span class="pi">:</span> <span class="s">random-generator</span>
 <span class="na">tolerations</span><span class="pi">:</span>
<span class="c1"># Tolerate (i.e., consider for scheduling) nodes, which have a taint with key noderole.kubernetes.io/master. On production clusters, this taint is set on the master node to prevent scheduling of Pods on the master. A toleration like this allows this Pod to be installed on the master nevertheless.</span>
 <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">node-role.kubernetes.io/master</span>
<span class="c1"># Tolerate only when the taint specifies a NoSchedule effect. This field can be empty here, in which case the toleration applies to every effect.</span>
 <span class="na">operator</span><span class="pi">:</span> <span class="s">Exists</span>
 <span class="na">effect</span><span class="pi">:</span> <span class="s">NoSchedule</span>
</code></pre></div></div>

<h3 id="fabric8configmapcontroller-controller-script">fabric8/configmapcontroller (controller script)</h3>

<ul>
  <li>This is a controller that watches ConfigMap objects for changes and performs rolling upgrades of their associated Deployments. It can be used with applications that are not capable of watching the ConfigMap and updating themselves with new configurations dynamically. That is particularly true when a Pod consumes this ConfigMap as environment variables or when your application cannot quickly and reliably update itself on the fly without a restart.</li>
</ul>

<h2 id="aws-eks-example">AWS EKS Example</h2>

<p><img src="pics\aws_eks_architecture.jpg" alt="architecture" /></p>

<ul>
  <li>Install:
    <ul>
      <li>docker</li>
      <li>aws</li>
      <li>kubectl</li>
      <li>aws-iam-authenticatior (just move to folder in the PATH)</li>
      <li>eksctl (just move to folder in the PATH)</li>
    </ul>
  </li>
  <li>Create IAM Role with AmazonEKSClusterPolicy and AmazonEKSServicePolicy</li>
  <li>Create VPC with new stack https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml</li>
  <li>AWS WebUI: Create cluster with IAM and VPC (may take 10+ munites)
    <ul>
      <li>another option is<br />
 <code class="language-plaintext highlighter-rouge">&gt; eksctl create cluster --name &lt;NAME&gt; --nodes 3 --region &lt;REGION_ID&gt;</code><br />
 , which will also create the modes.</li>
      <li>another option is cloudformation stack:</li>
    </ul>
  </li>
</ul>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## --nodes, --region</span>
<span class="na">Type</span><span class="pi">:</span> <span class="s">AWS::EKS::Cluster</span>
<span class="na">Properties</span><span class="pi">:</span>
  <span class="na">Name</span><span class="pi">:</span> <span class="s">prod</span>
  <span class="na">Version</span><span class="pi">:</span> <span class="s1">'</span><span class="s">1.11'</span>
  <span class="na">RoleArn</span><span class="pi">:</span> <span class="s">arn:aws:iam::012345678910:role/eks-service-role-AWSServiceRoleForAmazonEKS-EXAMPLEBQ4PI</span>
  <span class="na">ResourcesVpcConfig</span><span class="pi">:</span>
    <span class="na">SecurityGroupIds</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">sg-6979fe18</span>
    <span class="na">SubnetIds</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">subnet-6782e71e</span>
    <span class="pi">-</span> <span class="s">subnet-e7e761ac</span>
</code></pre></div></div>

<ul>
  <li>Connect kubectl with cluster<br />
<code class="language-plaintext highlighter-rouge">&gt; aws eks update-kubeconfig --name &lt;CLUSTER_NAME&gt;</code></li>
  <li>Create worker nodes with new stack https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml
    <ul>
      <li>use <CLUSTER_NAME></CLUSTER_NAME></li>
      <li>use <SECURITY_GROUP> which is an output artefact from VPC</SECURITY_GROUP></li>
      <li>use NodeImageId from the table in the main link above</li>
      <li>keyPair - get from Services -&gt; EC2 -&gt; Network &amp; Security -&gt; Key Pairs -&gt; Create Key Pair</li>
      <li>vpcid and subnets from VPC</li>
    </ul>
  </li>
  <li>Connect nodes to cluster: <code class="language-plaintext highlighter-rouge">ConfigMap</code> object<br />
<code class="language-plaintext highlighter-rouge">&gt; kubectl apply -f aws-auth-cm.yml</code><br />
<code class="language-plaintext highlighter-rouge">&gt; kubectl get nodes</code> should print the nodes</li>
</ul>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html</span>
<span class="c1"># k8s aws-auth-cm.yml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ConfigMap</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">aws-auth</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">mapRoles</span><span class="pi">:</span>
    <span class="c1"># get it from the stack output for worker nodes creation</span>
    <span class="pi">-</span> <span class="na">rolearn</span><span class="pi">:</span> <span class="s">NodeInstanceRole from previous step output</span>
      <span class="na">username</span><span class="pi">:</span> <span class="s">system:node:</span>
      <span class="na">groups</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">system:bootstrappers</span>
        <span class="pi">-</span> <span class="s">system:nodes</span>
</code></pre></div></div>

<ul>
  <li>Launch an app (k8s object <code class="language-plaintext highlighter-rouge">Deployment</code>, <code class="language-plaintext highlighter-rouge">Service</code>)<br />
<code class="language-plaintext highlighter-rouge">&gt; kubectl create -f ....yml</code><br />
<code class="language-plaintext highlighter-rouge">&gt; kubectl get services -o wide</code> and get the EXTERNAL_IP address<br />
<code class="language-plaintext highlighter-rouge">&gt; kubectl describe svc &lt;SERVICE-NAME&gt;</code></li>
  <li>Delete everything:<br />
<code class="language-plaintext highlighter-rouge">&gt; kubectl get svc --all-namespaces</code><br />
<code class="language-plaintext highlighter-rouge">&gt; kubectl delete svc &lt;SERVICE_NAME&gt;</code><br />
<code class="language-plaintext highlighter-rouge">&gt; kubectl delete deployment &lt;DEPLOYMENT_NAME&gt;</code>\, will delete the rs too<br />
<code class="language-plaintext highlighter-rouge">&gt; kubectl get deployments</code> to find the proper name<br />
<code class="language-plaintext highlighter-rouge">&gt; kubectl get rs</code><br />
stop/terminate ec2 instances ???<br />
delete node creation stack<br />
delete cluster creations stack<br />
delete iam role</li>
  <li>ELB AWS configu ?? <code class="language-plaintext highlighter-rouge">&gt; aws iam get-role --role-name "AWSServiceRoleForElasticLoadBalancing" || aws iam create-service-linked-role --aws-service-name "elasticloadbalancing.amazonaws.com"</code></li>
</ul>

<h3 id="aws-eks-example-1">aws eks example</h3>

<p>https://eksworkshop.com<br />
https://github.com/brentley/ecsdemo-nodejs<br />
https://hub.docker.com/r/brentley/ecsdemo-nodejs<br />
https://docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html<br />
https://www.edureka.co/blog/amazon-eks/<br />
https://www.youtube.com/watch?v=6H5sXQoJiso<br />
https://www.oreilly.com/ideas/how-to-manage-docker-containers-in-kubernetes-with-java<br />
https://www.youtube.com/watch?v=8OPkt93WyPA</p>

<h2 id="references">References</h2>




<ul class="bibliography"><li><span id="noauthor0000oracle">Oracle Linux Container Services for use with Kubernetes User’s Guide. In <i>Oracle Help Center</i>. Retrieved January 31, 2021, from https://docs.oracle.com/en/operating-systems/oracle-linux/kubernetes/</span></li>
<li><span id="burns2019kubernetes">Burns, B., Hightower, K., &amp; Beda, J. (2019). <i>Kubernetes: up and running</i>.</span></li>
<li><span id="dinesh2018k8sbestpractices">Dinesh, S. (2018). <i>Kubernetes Best Practices</i>. https://cloud.google.com/blog/topics/kubernetes-best-practices</span></li>
<li><span id="gupta2017kubernetes">Gupta, A. (2017). <i>Kubernetes for Java developers</i>.</span></li>
<li><span id="bilgin2019kubernetes">Ibryam, B., &amp; Hub, R. (2019). <i>Kubernetes Patterns</i>. O’Reilly Media.</span></li>
<li><span id="krochmalski2017docker">Krochmalski, J. (2017). <i>Docker and Kubernetes for Java Developers</i>. Packt Publishing.</span></li></ul>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/"></data>
  
    <div class="wrapper">
  
      <div class="footer-col-wrapper">
        <div class="footer-col">
        <!--
          <p class="feed-subscribe">
            <a href="/feed.xml">
              <svg class="svg-icon orange">
                <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
              </svg><span>Subscribe</span>
            </a>
          </p>
        -->
          <ul class="contact-list">
            <li class="p-name">emodemo</li>
            
          </ul>
        </div>
        <div class="footer-col">
          <p>emodemo&#39;s notes on development, fiction, science and other stuff.</p>
        </div>
      </div>
  
      <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/emodemo" title="emodemo"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/emiliyan-todorov" title="emiliyan-todorov"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/realemodemo" title="realemodemo"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>
  
    </div>
  
  </footer></body>

</html>
